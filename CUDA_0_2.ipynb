{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUDy5dpd3euGqxwVMRbFHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag005/CUDA-Kernel-project/blob/main/CUDA_0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cjZxyWcwf5L",
        "outputId": "35ec4266-7e23-4bab-8ef7-f1aaa67d8aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sun Oct 26 18:29:37 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "Device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Enable GPU Runtime\n",
        "# Go to: Runtime > Change runtime type > Hardware accelerator > GPU > Save\n",
        "\n",
        "# Cell 2: Verify CUDA availability\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install nvcc4jupyter\n",
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AmQ0VxRyN0I",
        "outputId": "a5029e39-8acd-45a5-a707-bb4ac0a732ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp210u15u5\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Custom ReLU CUDA Kernel\n",
        "%load_ext nvcc4jupyter\n",
        "%%cu\n",
        "#include <torch/extension.h>\n",
        "\n",
        "__global__ void relu_cuda_kernel(const float* input, float* output, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = input[idx] > 0.0f ? input[idx] : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor relu_cuda_forward(torch::Tensor input) {\n",
        "    const int size = input.numel();\n",
        "    auto output = torch::empty_like(input);\n",
        "\n",
        "    const int threads = 256;\n",
        "    const int blocks = (size + threads - 1) / threads;\n",
        "\n",
        "    relu_cuda_kernel<<<blocks, threads>>>(\n",
        "        input.data_ptr<float>(),\n",
        "        output.data_ptr<float>(),\n",
        "        size\n",
        "    );\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "di0FNxRSyTSA",
        "outputId": "430ec2e2-035e-4572-92f4-174546254b1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (ipython-input-3793857798.py, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3793857798.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    output[idx] = input[idx] > 0.0f ? input[idx] : 0.0f;\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Compiling Custom CUDA Kernel\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "# CUDA kernel source code\n",
        "cuda_source = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "\n",
        "__global__ void relu_cuda_kernel(const float* input, float* output, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = input[idx] > 0.0f ? input[idx] : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor relu_forward(torch::Tensor input) {\n",
        "    const int size = input.numel();\n",
        "    auto output = torch::empty_like(input);\n",
        "\n",
        "    const int threads = 256;\n",
        "    const int blocks = (size + threads - 1) / threads;\n",
        "\n",
        "    relu_cuda_kernel<<<blocks, threads>>>(\n",
        "        input.data_ptr<float>(),\n",
        "        output.data_ptr<float>(),\n",
        "        size\n",
        "    );\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "cpp_source = \"\"\"\n",
        "torch::Tensor relu_forward(torch::Tensor input);\n",
        "\"\"\"\n",
        "\n",
        "# Compile the extension\n",
        "print(\"\\nüî® Compiling CUDA extension...\")\n",
        "relu_cuda = load_inline(\n",
        "    name='relu_cuda',\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_source,\n",
        "    functions=['relu_forward'],\n",
        "    verbose=False,\n",
        "    extra_cflags=['-O3'],\n",
        "    extra_cuda_cflags=['-O3', '--use_fast_math']\n",
        ")\n",
        "print(\"‚úÖ CUDA extension compiled successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rV1OzcLzonQ",
        "outputId": "041e2e4e-0abe-4123-b69e-3a348338e1ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1026 18:37:35.468000 424 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W1026 18:37:35.468000 424 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: Compiling Custom CUDA Kernel\n",
            "================================================================================\n",
            "\n",
            "üî® Compiling CUDA extension...\n",
            "‚úÖ CUDA extension compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d13efdd4",
        "outputId": "af934c21-cbfa-4c3c-bada-0ee8a97f02de"
      },
      "source": [
        "# Install ninja\n",
        "!pip install ninja"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Testing Correctness\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test with simple values\n",
        "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], device='cuda')\n",
        "custom_output = relu_cuda.relu_forward(test_input)\n",
        "pytorch_output = torch.relu(test_input)\n",
        "\n",
        "print(f\"\\nüß™ Test Input:     {test_input.cpu().numpy()}\")\n",
        "print(f\"‚úÖ Custom Output:  {custom_output.cpu().numpy()}\")\n",
        "print(f\"‚úÖ PyTorch Output: {pytorch_output.cpu().numpy()}\")\n",
        "print(f\"‚úÖ Outputs Match:  {torch.allclose(custom_output, pytorch_output)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98oHGMym0dAz",
        "outputId": "106363ff-e628-4e59-85d6-0c5b7ef73216"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: Testing Correctness\n",
            "================================================================================\n",
            "\n",
            "üß™ Test Input:     [-2. -1.  0.  1.  2.]\n",
            "‚úÖ Custom Output:  [0. 0. 0. 1. 2.]\n",
            "‚úÖ PyTorch Output: [0. 0. 0. 1. 2.]\n",
            "‚úÖ Outputs Match:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: Performance Benchmarking\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def benchmark(size=1000000, iterations=100):\n",
        "    input_tensor = torch.randn(size, device='cuda')\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = relu_cuda.relu_forward(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Custom ReLU timing\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        output = relu_cuda.relu_forward(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    custom_time = (time.time() - start) / iterations\n",
        "\n",
        "    # PyTorch ReLU timing\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        output = torch.relu(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    pytorch_time = (time.time() - start) / iterations\n",
        "\n",
        "    print(f\"\\nüìä Benchmark Results (Size: {size:,} elements, {iterations} iterations):\")\n",
        "    print(f\"   Custom CUDA ReLU:  {custom_time*1000:.4f} ms\")\n",
        "    print(f\"   PyTorch ReLU:      {pytorch_time*1000:.4f} ms\")\n",
        "    print(f\"   Speedup:           {pytorch_time/custom_time:.2f}x\")\n",
        "\n",
        "    return custom_time, pytorch_time\n",
        "\n",
        "# Run benchmarks with different sizes\n",
        "for size in [100000, 1000000, 10000000]:\n",
        "    benchmark(size=size, iterations=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6ZDqs810kl7",
        "outputId": "39d435b4-1bdc-4af3-d1e9-29e91261f722"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: Performance Benchmarking\n",
            "================================================================================\n",
            "\n",
            "üìä Benchmark Results (Size: 100,000 elements, 50 iterations):\n",
            "   Custom CUDA ReLU:  0.0111 ms\n",
            "   PyTorch ReLU:      0.0112 ms\n",
            "   Speedup:           1.01x\n",
            "\n",
            "üìä Benchmark Results (Size: 1,000,000 elements, 50 iterations):\n",
            "   Custom CUDA ReLU:  0.0380 ms\n",
            "   PyTorch ReLU:      0.0366 ms\n",
            "   Speedup:           0.96x\n",
            "\n",
            "üìä Benchmark Results (Size: 10,000,000 elements, 50 iterations):\n",
            "   Custom CUDA ReLU:  0.3381 ms\n",
            "   PyTorch ReLU:      0.3375 ms\n",
            "   Speedup:           1.00x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: PyTorch Profiler Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "\n",
        "input_tensor = torch.randn(1000000, device='cuda')\n",
        "\n",
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    record_shapes=True,\n",
        ") as prof:\n",
        "    for _ in range(10):\n",
        "        output = relu_cuda.relu_forward(input_tensor)\n",
        "\n",
        "print(\"\\nüîç Profiler Results (Top 10 by CUDA time):\")\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98_Hd-U20oCJ",
        "outputId": "c6e1e211-61e9-4d1a-9347-5e70d570c480"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 5: PyTorch Profiler Analysis\n",
            "================================================================================\n",
            "\n",
            "üîç Profiler Results (Top 10 by CUDA time):\n",
            "-----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "    relu_cuda_kernel(float const*, float*, int)         0.00%       0.000us         0.00%       0.000us       0.000us     414.644us       100.00%     414.644us      41.464us            10  \n",
            "                               aten::empty_like        49.03%       2.811ms        94.73%       5.431ms     543.096us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                            aten::empty_strided         2.06%     118.043us        45.70%       2.620ms     262.004us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                                   Unrecognized        43.64%       2.502ms        43.64%       2.502ms       2.502ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                               cudaLaunchKernel         1.92%     110.086us         1.92%     110.086us      11.009us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                          cudaDeviceSynchronize         3.35%     192.180us         3.35%     192.180us     192.180us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 5.733ms\n",
            "Self CUDA time total: 414.644us\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: Optimized CUDA Kernel (Vectorized with float4)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cuda_source_optimized = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "\n",
        "__global__ void relu_cuda_kernel_optimized(const float* input, float* output, int size) {\n",
        "    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n",
        "\n",
        "    // Process 4 elements at once using float4\n",
        "    if (idx + 3 < size) {\n",
        "        float4 val = reinterpret_cast<const float4*>(input)[idx/4];\n",
        "        float4 result;\n",
        "        result.x = val.x > 0.0f ? val.x : 0.0f;\n",
        "        result.y = val.y > 0.0f ? val.y : 0.0f;\n",
        "        result.z = val.z > 0.0f ? val.z : 0.0f;\n",
        "        result.w = val.w > 0.0f ? val.w : 0.0f;\n",
        "        reinterpret_cast<float4*>(output)[idx/4] = result;\n",
        "    }\n",
        "\n",
        "    // Handle remaining elements\n",
        "    for (int i = (size/4)*4 + threadIdx.x; i < size; i += blockDim.x) {\n",
        "        if (i < size) {\n",
        "            output[i] = input[i] > 0.0f ? input[i] : 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor relu_forward_optimized(torch::Tensor input) {\n",
        "    const int size = input.numel();\n",
        "    auto output = torch::empty_like(input);\n",
        "\n",
        "    const int threads = 256;\n",
        "    const int blocks = (size/4 + threads - 1) / threads;\n",
        "\n",
        "    relu_cuda_kernel_optimized<<<blocks, threads>>>(\n",
        "        input.data_ptr<float>(),\n",
        "        output.data_ptr<float>(),\n",
        "        size\n",
        "    );\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüî® Compiling optimized CUDA kernel...\")\n",
        "relu_cuda_opt = load_inline(\n",
        "    name='relu_cuda_opt',\n",
        "    cpp_sources=\"torch::Tensor relu_forward_optimized(torch::Tensor input);\",\n",
        "    cuda_sources=cuda_source_optimized,\n",
        "    functions=['relu_forward_optimized'],\n",
        "    verbose=False,\n",
        "    extra_cuda_cflags=['-O3', '--use_fast_math']\n",
        ")\n",
        "print(\"‚úÖ Optimized CUDA extension compiled successfully!\")\n",
        "\n",
        "# Test optimized version\n",
        "test_input = torch.randn(1000000, device='cuda')\n",
        "opt_output = relu_cuda_opt.relu_forward_optimized(test_input)\n",
        "pytorch_output = torch.relu(test_input)\n",
        "print(f\"‚úÖ Optimized version correctness: {torch.allclose(opt_output, pytorch_output, atol=1e-5)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eEsrIOu0vYu",
        "outputId": "a5d984c8-da53-420a-c3fc-1826fc7836cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1026 18:41:07.690000 424 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W1026 18:41:07.690000 424 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: Optimized CUDA Kernel (Vectorized with float4)\n",
            "================================================================================\n",
            "\n",
            "üî® Compiling optimized CUDA kernel...\n",
            "‚úÖ Optimized CUDA extension compiled successfully!\n",
            "‚úÖ Optimized version correctness: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: Performance Comparison (All Versions)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def benchmark_all_versions(size=10000000, iterations=50):\n",
        "    input_tensor = torch.randn(size, device='cuda')\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = relu_cuda.relu_forward(input_tensor)\n",
        "        _ = relu_cuda_opt.relu_forward_optimized(input_tensor)\n",
        "        _ = torch.relu(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Naive custom\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = relu_cuda.relu_forward(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    naive_time = (time.time() - start) / iterations\n",
        "\n",
        "    # Optimized custom\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = relu_cuda_opt.relu_forward_optimized(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    opt_time = (time.time() - start) / iterations\n",
        "\n",
        "    # PyTorch native\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = torch.relu(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    pytorch_time = (time.time() - start) / iterations\n",
        "\n",
        "    print(f\"\\nüìä Final Benchmark (Size: {size:,} elements):\")\n",
        "    print(f\"   {'Method':<30} {'Time (ms)':<15} {'Speedup':<10}\")\n",
        "    print(f\"   {'-'*55}\")\n",
        "    print(f\"   {'Naive Custom CUDA':<30} {naive_time*1000:>10.4f} ms   {pytorch_time/naive_time:>6.2f}x\")\n",
        "    print(f\"   {'Optimized CUDA (float4)':<30} {opt_time*1000:>10.4f} ms   {pytorch_time/opt_time:>6.2f}x\")\n",
        "    print(f\"   {'PyTorch Native':<30} {pytorch_time*1000:>10.4f} ms   {'1.00x':>10}\")\n",
        "    print(f\"\\n   üöÄ Optimization gain: {naive_time/opt_time:.2f}x faster than naive version\")\n",
        "\n",
        "benchmark_all_versions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KRfhfqK2ysh",
        "outputId": "8cb8af7b-5f20-46b5-d666-e7f72532331c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 7: Performance Comparison (All Versions)\n",
            "================================================================================\n",
            "\n",
            "üìä Final Benchmark (Size: 10,000,000 elements):\n",
            "   Method                         Time (ms)       Speedup   \n",
            "   -------------------------------------------------------\n",
            "   Naive Custom CUDA                  0.3365 ms     1.01x\n",
            "   Optimized CUDA (float4)            0.3311 ms     1.02x\n",
            "   PyTorch Native                     0.3385 ms        1.00x\n",
            "\n",
            "   üöÄ Optimization gain: 1.02x faster than naive version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: Memory Usage Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "size = 10000000\n",
        "input_tensor = torch.randn(size, device='cuda')\n",
        "output = relu_cuda.relu_forward(input_tensor)\n",
        "\n",
        "print(f\"\\nüìà Memory Statistics (for {size:,} elements):\")\n",
        "print(f\"   Input size:       {input_tensor.element_size() * input_tensor.nelement() / 1024**2:.2f} MB\")\n",
        "print(f\"   Output size:      {output.element_size() * output.nelement() / 1024**2:.2f} MB\")\n",
        "print(f\"   Allocated memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"   Reserved memory:  {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
        "print(f\"   Peak memory:      {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8GybplP2hOW",
        "outputId": "00686f95-cb7e-4cc5-a924-2430c693bbd3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: Memory Usage Analysis\n",
            "================================================================================\n",
            "\n",
            "üìà Memory Statistics (for 10,000,000 elements):\n",
            "   Input size:       38.15 MB\n",
            "   Output size:      38.15 MB\n",
            "   Allocated memory: 88.67 MB\n",
            "   Reserved memory:  102.00 MB\n",
            "   Peak memory:      92.48 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚úÖ Successfully implemented custom CUDA ReLU kernel in PyTorch\n",
        "‚úÖ Compiled and tested on GPU\n",
        "‚úÖ Benchmarked performance against PyTorch native implementation\n",
        "‚úÖ Profiled with PyTorch Profiler\n",
        "‚úÖ Implemented optimized version with vectorized memory access (float4)\n",
        "‚úÖ Analyzed memory usage patterns\n",
        "\n",
        "Key Achievements:\n",
        "- Custom CUDA kernel integration with PyTorch\n",
        "- Performance profiling and bottleneck identification\n",
        "- Code optimization with vectorization\n",
        "- Memory-efficient GPU computation\n",
        "\n",
        "Skills Demonstrated:\n",
        "- PyTorch + CUDA programming\n",
        "- GPU kernel optimization\n",
        "- Performance profiling and analysis\n",
        "- Memory management on GPU\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üéâ CUDA KERNEL PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbNMGDqi2mr4",
        "outputId": "d646d8c3-b755-4b82-fe5d-62adcc5aeec3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROJECT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Successfully implemented custom CUDA ReLU kernel in PyTorch\n",
            "‚úÖ Compiled and tested on GPU\n",
            "‚úÖ Benchmarked performance against PyTorch native implementation\n",
            "‚úÖ Profiled with PyTorch Profiler\n",
            "‚úÖ Implemented optimized version with vectorized memory access (float4)\n",
            "‚úÖ Analyzed memory usage patterns\n",
            "\n",
            "Key Achievements:\n",
            "- Custom CUDA kernel integration with PyTorch\n",
            "- Performance profiling and bottleneck identification\n",
            "- Code optimization with vectorization\n",
            "- Memory-efficient GPU computation\n",
            "\n",
            "Skills Demonstrated:\n",
            "- PyTorch + CUDA programming\n",
            "- GPU kernel optimization\n",
            "- Performance profiling and analysis\n",
            "- Memory management on GPU\n",
            "\n",
            "================================================================================\n",
            "üéâ CUDA KERNEL PROJECT COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}