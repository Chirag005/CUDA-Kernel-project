{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUDy5dpd3euGqxwVMRbFHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag005/CUDA-Kernel-project/blob/main/CUDA_0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cjZxyWcwf5L",
        "outputId": "35ec4266-7e23-4bab-8ef7-f1aaa67d8aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sun Oct 26 18:29:37 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "Device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Enable GPU Runtime\n",
        "# Go to: Runtime > Change runtime type > Hardware accelerator > GPU > Save\n",
        "\n",
        "# Cell 2: Verify CUDA availability\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install nvcc4jupyter\n",
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AmQ0VxRyN0I",
        "outputId": "a5029e39-8acd-45a5-a707-bb4ac0a732ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp210u15u5\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Custom ReLU CUDA Kernel\n",
        "%load_ext nvcc4jupyter\n",
        "%%cu\n",
        "#include <torch/extension.h>\n",
        "\n",
        "__global__ void relu_cuda_kernel(const float* input, float* output, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = input[idx] > 0.0f ? input[idx] : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor relu_cuda_forward(torch::Tensor input) {\n",
        "    const int size = input.numel();\n",
        "    auto output = torch::empty_like(input);\n",
        "\n",
        "    const int threads = 256;\n",
        "    const int blocks = (size + threads - 1) / threads;\n",
        "\n",
        "    relu_cuda_kernel<<<blocks, threads>>>(\n",
        "        input.data_ptr<float>(),\n",
        "        output.data_ptr<float>(),\n",
        "        size\n",
        "    );\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "di0FNxRSyTSA",
        "outputId": "430ec2e2-035e-4572-92f4-174546254b1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (ipython-input-3793857798.py, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3793857798.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    output[idx] = input[idx] > 0.0f ? input[idx] : 0.0f;\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Compiling Custom CUDA Kernel\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "# CUDA kernel source code\n",
        "cuda_source = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "\n",
        "__global__ void relu_cuda_kernel(const float* input, float* output, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = input[idx] > 0.0f ? input[idx] : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor relu_forward(torch::Tensor input) {\n",
        "    const int size = input.numel();\n",
        "    auto output = torch::empty_like(input);\n",
        "\n",
        "    const int threads = 256;\n",
        "    const int blocks = (size + threads - 1) / threads;\n",
        "\n",
        "    relu_cuda_kernel<<<blocks, threads>>>(\n",
        "        input.data_ptr<float>(),\n",
        "        output.data_ptr<float>(),\n",
        "        size\n",
        "    );\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "cpp_source = \"\"\"\n",
        "torch::Tensor relu_forward(torch::Tensor input);\n",
        "\"\"\"\n",
        "\n",
        "# Compile the extension\n",
        "print(\"\\n🔨 Compiling CUDA extension...\")\n",
        "relu_cuda = load_inline(\n",
        "    name='relu_cuda',\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_source,\n",
        "    functions=['relu_forward'],\n",
        "    verbose=False,\n",
        "    extra_cflags=['-O3'],\n",
        "    extra_cuda_cflags=['-O3', '--use_fast_math']\n",
        ")\n",
        "print(\"✅ CUDA extension compiled successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rV1OzcLzonQ",
        "outputId": "041e2e4e-0abe-4123-b69e-3a348338e1ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1026 18:37:35.468000 424 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W1026 18:37:35.468000 424 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: Compiling Custom CUDA Kernel\n",
            "================================================================================\n",
            "\n",
            "🔨 Compiling CUDA extension...\n",
            "✅ CUDA extension compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d13efdd4",
        "outputId": "af934c21-cbfa-4c3c-bada-0ee8a97f02de"
      },
      "source": [
        "# Install ninja\n",
        "!pip install ninja"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Testing Correctness\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test with simple values\n",
        "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], device='cuda')\n",
        "custom_output = relu_cuda.relu_forward(test_input)\n",
        "pytorch_output = torch.relu(test_input)\n",
        "\n",
        "print(f\"\\n🧪 Test Input:     {test_input.cpu().numpy()}\")\n",
        "print(f\"✅ Custom Output:  {custom_output.cpu().numpy()}\")\n",
        "print(f\"✅ PyTorch Output: {pytorch_output.cpu().numpy()}\")\n",
        "print(f\"✅ Outputs Match:  {torch.allclose(custom_output, pytorch_output)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98oHGMym0dAz",
        "outputId": "106363ff-e628-4e59-85d6-0c5b7ef73216"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: Testing Correctness\n",
            "================================================================================\n",
            "\n",
            "🧪 Test Input:     [-2. -1.  0.  1.  2.]\n",
            "✅ Custom Output:  [0. 0. 0. 1. 2.]\n",
            "✅ PyTorch Output: [0. 0. 0. 1. 2.]\n",
            "✅ Outputs Match:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: Performance Benchmarking\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def benchmark(size=1000000, iterations=100):\n",
        "    input_tensor = torch.randn(size, device='cuda')\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = relu_cuda.relu_forward(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Custom ReLU timing\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        output = relu_cuda.relu_forward(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    custom_time = (time.time() - start) / iterations\n",
        "\n",
        "    # PyTorch ReLU timing\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        output = torch.relu(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    pytorch_time = (time.time() - start) / iterations\n",
        "\n",
        "    print(f\"\\n📊 Benchmark Results (Size: {size:,} elements, {iterations} iterations):\")\n",
        "    print(f\"   Custom CUDA ReLU:  {custom_time*1000:.4f} ms\")\n",
        "    print(f\"   PyTorch ReLU:      {pytorch_time*1000:.4f} ms\")\n",
        "    print(f\"   Speedup:           {pytorch_time/custom_time:.2f}x\")\n",
        "\n",
        "    return custom_time, pytorch_time\n",
        "\n",
        "# Run benchmarks with different sizes\n",
        "for size in [100000, 1000000, 10000000]:\n",
        "    benchmark(size=size, iterations=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6ZDqs810kl7",
        "outputId": "39d435b4-1bdc-4af3-d1e9-29e91261f722"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: Performance Benchmarking\n",
            "================================================================================\n",
            "\n",
            "📊 Benchmark Results (Size: 100,000 elements, 50 iterations):\n",
            "   Custom CUDA ReLU:  0.0111 ms\n",
            "   PyTorch ReLU:      0.0112 ms\n",
            "   Speedup:           1.01x\n",
            "\n",
            "📊 Benchmark Results (Size: 1,000,000 elements, 50 iterations):\n",
            "   Custom CUDA ReLU:  0.0380 ms\n",
            "   PyTorch ReLU:      0.0366 ms\n",
            "   Speedup:           0.96x\n",
            "\n",
            "📊 Benchmark Results (Size: 10,000,000 elements, 50 iterations):\n",
            "   Custom CUDA ReLU:  0.3381 ms\n",
            "   PyTorch ReLU:      0.3375 ms\n",
            "   Speedup:           1.00x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: PyTorch Profiler Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "\n",
        "input_tensor = torch.randn(1000000, device='cuda')\n",
        "\n",
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    record_shapes=True,\n",
        ") as prof:\n",
        "    for _ in range(10):\n",
        "        output = relu_cuda.relu_forward(input_tensor)\n",
        "\n",
        "print(\"\\n🔍 Profiler Results (Top 10 by CUDA time):\")\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98_Hd-U20oCJ",
        "outputId": "c6e1e211-61e9-4d1a-9347-5e70d570c480"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 5: PyTorch Profiler Analysis\n",
            "================================================================================\n",
            "\n",
            "🔍 Profiler Results (Top 10 by CUDA time):\n",
            "-----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "    relu_cuda_kernel(float const*, float*, int)         0.00%       0.000us         0.00%       0.000us       0.000us     414.644us       100.00%     414.644us      41.464us            10  \n",
            "                               aten::empty_like        49.03%       2.811ms        94.73%       5.431ms     543.096us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                            aten::empty_strided         2.06%     118.043us        45.70%       2.620ms     262.004us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                                   Unrecognized        43.64%       2.502ms        43.64%       2.502ms       2.502ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                               cudaLaunchKernel         1.92%     110.086us         1.92%     110.086us      11.009us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                          cudaDeviceSynchronize         3.35%     192.180us         3.35%     192.180us     192.180us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 5.733ms\n",
            "Self CUDA time total: 414.644us\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: Optimized CUDA Kernel (Vectorized with float4)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cuda_source_optimized = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "\n",
        "__global__ void relu_cuda_kernel_optimized(const float* input, float* output, int size) {\n",
        "    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n",
        "\n",
        "    // Process 4 elements at once using float4\n",
        "    if (idx + 3 < size) {\n",
        "        float4 val = reinterpret_cast<const float4*>(input)[idx/4];\n",
        "        float4 result;\n",
        "        result.x = val.x > 0.0f ? val.x : 0.0f;\n",
        "        result.y = val.y > 0.0f ? val.y : 0.0f;\n",
        "        result.z = val.z > 0.0f ? val.z : 0.0f;\n",
        "        result.w = val.w > 0.0f ? val.w : 0.0f;\n",
        "        reinterpret_cast<float4*>(output)[idx/4] = result;\n",
        "    }\n",
        "\n",
        "    // Handle remaining elements\n",
        "    for (int i = (size/4)*4 + threadIdx.x; i < size; i += blockDim.x) {\n",
        "        if (i < size) {\n",
        "            output[i] = input[i] > 0.0f ? input[i] : 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor relu_forward_optimized(torch::Tensor input) {\n",
        "    const int size = input.numel();\n",
        "    auto output = torch::empty_like(input);\n",
        "\n",
        "    const int threads = 256;\n",
        "    const int blocks = (size/4 + threads - 1) / threads;\n",
        "\n",
        "    relu_cuda_kernel_optimized<<<blocks, threads>>>(\n",
        "        input.data_ptr<float>(),\n",
        "        output.data_ptr<float>(),\n",
        "        size\n",
        "    );\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n🔨 Compiling optimized CUDA kernel...\")\n",
        "relu_cuda_opt = load_inline(\n",
        "    name='relu_cuda_opt',\n",
        "    cpp_sources=\"torch::Tensor relu_forward_optimized(torch::Tensor input);\",\n",
        "    cuda_sources=cuda_source_optimized,\n",
        "    functions=['relu_forward_optimized'],\n",
        "    verbose=False,\n",
        "    extra_cuda_cflags=['-O3', '--use_fast_math']\n",
        ")\n",
        "print(\"✅ Optimized CUDA extension compiled successfully!\")\n",
        "\n",
        "# Test optimized version\n",
        "test_input = torch.randn(1000000, device='cuda')\n",
        "opt_output = relu_cuda_opt.relu_forward_optimized(test_input)\n",
        "pytorch_output = torch.relu(test_input)\n",
        "print(f\"✅ Optimized version correctness: {torch.allclose(opt_output, pytorch_output, atol=1e-5)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eEsrIOu0vYu",
        "outputId": "a5d984c8-da53-420a-c3fc-1826fc7836cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1026 18:41:07.690000 424 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W1026 18:41:07.690000 424 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: Optimized CUDA Kernel (Vectorized with float4)\n",
            "================================================================================\n",
            "\n",
            "🔨 Compiling optimized CUDA kernel...\n",
            "✅ Optimized CUDA extension compiled successfully!\n",
            "✅ Optimized version correctness: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: Performance Comparison (All Versions)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def benchmark_all_versions(size=10000000, iterations=50):\n",
        "    input_tensor = torch.randn(size, device='cuda')\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = relu_cuda.relu_forward(input_tensor)\n",
        "        _ = relu_cuda_opt.relu_forward_optimized(input_tensor)\n",
        "        _ = torch.relu(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Naive custom\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = relu_cuda.relu_forward(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    naive_time = (time.time() - start) / iterations\n",
        "\n",
        "    # Optimized custom\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = relu_cuda_opt.relu_forward_optimized(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    opt_time = (time.time() - start) / iterations\n",
        "\n",
        "    # PyTorch native\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = torch.relu(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    pytorch_time = (time.time() - start) / iterations\n",
        "\n",
        "    print(f\"\\n📊 Final Benchmark (Size: {size:,} elements):\")\n",
        "    print(f\"   {'Method':<30} {'Time (ms)':<15} {'Speedup':<10}\")\n",
        "    print(f\"   {'-'*55}\")\n",
        "    print(f\"   {'Naive Custom CUDA':<30} {naive_time*1000:>10.4f} ms   {pytorch_time/naive_time:>6.2f}x\")\n",
        "    print(f\"   {'Optimized CUDA (float4)':<30} {opt_time*1000:>10.4f} ms   {pytorch_time/opt_time:>6.2f}x\")\n",
        "    print(f\"   {'PyTorch Native':<30} {pytorch_time*1000:>10.4f} ms   {'1.00x':>10}\")\n",
        "    print(f\"\\n   🚀 Optimization gain: {naive_time/opt_time:.2f}x faster than naive version\")\n",
        "\n",
        "benchmark_all_versions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KRfhfqK2ysh",
        "outputId": "8cb8af7b-5f20-46b5-d666-e7f72532331c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 7: Performance Comparison (All Versions)\n",
            "================================================================================\n",
            "\n",
            "📊 Final Benchmark (Size: 10,000,000 elements):\n",
            "   Method                         Time (ms)       Speedup   \n",
            "   -------------------------------------------------------\n",
            "   Naive Custom CUDA                  0.3365 ms     1.01x\n",
            "   Optimized CUDA (float4)            0.3311 ms     1.02x\n",
            "   PyTorch Native                     0.3385 ms        1.00x\n",
            "\n",
            "   🚀 Optimization gain: 1.02x faster than naive version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: Memory Usage Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "size = 10000000\n",
        "input_tensor = torch.randn(size, device='cuda')\n",
        "output = relu_cuda.relu_forward(input_tensor)\n",
        "\n",
        "print(f\"\\n📈 Memory Statistics (for {size:,} elements):\")\n",
        "print(f\"   Input size:       {input_tensor.element_size() * input_tensor.nelement() / 1024**2:.2f} MB\")\n",
        "print(f\"   Output size:      {output.element_size() * output.nelement() / 1024**2:.2f} MB\")\n",
        "print(f\"   Allocated memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "print(f\"   Reserved memory:  {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
        "print(f\"   Peak memory:      {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8GybplP2hOW",
        "outputId": "00686f95-cb7e-4cc5-a924-2430c693bbd3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: Memory Usage Analysis\n",
            "================================================================================\n",
            "\n",
            "📈 Memory Statistics (for 10,000,000 elements):\n",
            "   Input size:       38.15 MB\n",
            "   Output size:      38.15 MB\n",
            "   Allocated memory: 88.67 MB\n",
            "   Reserved memory:  102.00 MB\n",
            "   Peak memory:      92.48 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        "✅ Successfully implemented custom CUDA ReLU kernel in PyTorch\n",
        "✅ Compiled and tested on GPU\n",
        "✅ Benchmarked performance against PyTorch native implementation\n",
        "✅ Profiled with PyTorch Profiler\n",
        "✅ Implemented optimized version with vectorized memory access (float4)\n",
        "✅ Analyzed memory usage patterns\n",
        "\n",
        "Key Achievements:\n",
        "- Custom CUDA kernel integration with PyTorch\n",
        "- Performance profiling and bottleneck identification\n",
        "- Code optimization with vectorization\n",
        "- Memory-efficient GPU computation\n",
        "\n",
        "Skills Demonstrated:\n",
        "- PyTorch + CUDA programming\n",
        "- GPU kernel optimization\n",
        "- Performance profiling and analysis\n",
        "- Memory management on GPU\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"🎉 CUDA KERNEL PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbNMGDqi2mr4",
        "outputId": "d646d8c3-b755-4b82-fe5d-62adcc5aeec3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROJECT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "✅ Successfully implemented custom CUDA ReLU kernel in PyTorch\n",
            "✅ Compiled and tested on GPU\n",
            "✅ Benchmarked performance against PyTorch native implementation\n",
            "✅ Profiled with PyTorch Profiler\n",
            "✅ Implemented optimized version with vectorized memory access (float4)\n",
            "✅ Analyzed memory usage patterns\n",
            "\n",
            "Key Achievements:\n",
            "- Custom CUDA kernel integration with PyTorch\n",
            "- Performance profiling and bottleneck identification\n",
            "- Code optimization with vectorization\n",
            "- Memory-efficient GPU computation\n",
            "\n",
            "Skills Demonstrated:\n",
            "- PyTorch + CUDA programming\n",
            "- GPU kernel optimization\n",
            "- Performance profiling and analysis\n",
            "- Memory management on GPU\n",
            "\n",
            "================================================================================\n",
            "🎉 CUDA KERNEL PROJECT COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}